{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcbbb6d-0c54-463c-9939-4bea284a4c44",
   "metadata": {},
   "source": [
    "# Differential Equations\n",
    "\n",
    "- https://nextjournal.com/sosiris-de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4c197-5d42-46ce-9334-236c139c5e6a",
   "metadata": {},
   "source": [
    "In mathematics, a differential equation is an equation that relates one or more unknown functions and their derivatives. [wikipedia](https://en.wikipedia.org/wiki/Differential_equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e3d1e-8d48-477c-8020-d8cfb9eaf752",
   "metadata": {},
   "source": [
    "## Ordinary Differential Equation (ODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3367b7-a41d-4d7d-9f0e-604e1e3a738f",
   "metadata": {},
   "source": [
    "We define an ordinary differential equation as an equation which describes the way that a variable u changes, that is\n",
    "\n",
    "$$\n",
    "u' = f(u, p, t)\n",
    "$$\n",
    "\n",
    "where $p$ are the parameters of the model, $t$ is the time variable, and $f$ is the nonlinear model of how $u$ changes. The initial value problem also includes the information about the starting value:\n",
    "\n",
    "$$\n",
    "u(t_0) = u_0\n",
    "$$\n",
    "\n",
    "Together, if you know the starting value and you know how the value will change with time, then you know what the value will be at any time point in the future. This is the intuitive definition of a differential equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038863fa-618a-4d02-b836-84a8a3de4961",
   "metadata": {},
   "source": [
    "For Python, check [scipy.integrate.solve_ivp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html).\n",
    "\n",
    "For Julia, check [DifferentialEquations.jl](https://docs.sciml.ai/DiffEqDocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f4774-3edd-4956-873d-4446c1d702b0",
   "metadata": {},
   "source": [
    "## Partial Differential Equation (PDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784e000-4baa-460e-8988-0e6171a5a613",
   "metadata": {},
   "source": [
    "A partial differential equation is a differential equation which has partial derivatives. \n",
    "\n",
    "**The best way to solve a PDE is...**\n",
    "\n",
    "**By converting it into another problem!**\n",
    "\n",
    "Generally, PDEs are converted into:\n",
    "\n",
    "- Linear systems: `Ax = b`, find `x`.\n",
    "- Nonlinear systems: `G(x) = 0`, find `x`.\n",
    "- ODEs: `u' = f(u,p,t)`, find `u`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e38ee59-e161-4db1-a319-0463ca7455ad",
   "metadata": {},
   "source": [
    "There are thus 4 types of packages in the PDE solver pipeline:\n",
    "1. Packages with ways to represent functions as vectors of numbers and their derivatives as matrices\n",
    "    - There are four main ways:\n",
    "        - Finite difference method (FDM): functions are represented on a grid.\n",
    "        - Finite volume method (FVM): functions are represented by a discretization of its integral. \n",
    "        - Finite element method (FEM): functions are represented by a local basis. \n",
    "        - Spectral methods: functions are represented by a global basis.\n",
    "2. Packages which solve linear systems\n",
    "3. Packages which solve nonlinear rootfinding problems\n",
    "4. Packages which solve ODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511aba1-937f-456e-aaad-4a51c3b33231",
   "metadata": {},
   "source": [
    "You need discretization tooling, linear solvers, nonlinear solvers, and finally ODE solvers to build an efficient PDE solver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31810dd-ebcd-4f2c-abf3-4b8199416efe",
   "metadata": {},
   "source": [
    "## Neural Differential Equations\n",
    "\n",
    "- https://arxiv.org/abs/1806.07366\n",
    "- https://arxiv.org/abs/2001.04385\n",
    "- https://arxiv.org/abs/2202.02435"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c92f60-7249-441f-b76a-87ca094b4ce5",
   "metadata": {},
   "source": [
    "### Neural DE\n",
    "\n",
    "The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs)\n",
    "demonstrate that **neural networks and differential equation are two sides of the same coin**.\n",
    "\n",
    "A neural differential equation is a differential equation using a neural network to parameterise the vector field. The canonical example is a **neural ordinary differential equation**:\n",
    "\n",
    "$$\n",
    "y(0) = y_0 \\qquad \\frac{\\mathrm{d} y}{\\mathrm{d} t}(t) = f_\\theta(t, y(t)).\n",
    "$$\n",
    "\n",
    "As a simple example, suppose we observe some picture $y_0 \\in \\mathbb{R}^{3 \\times 32 \\times 32}$ (RGB and $32 \\times 32$ pixels), and wish to classify it as a picture of a cat or as a picture of a dog.\n",
    "\n",
    "We proceed by taking $y(0) = y_0$ as the initial condition of the neural ODE, and evolve the ODE until some time $T$. An affine transformation $\\ell_\\theta \\colon \\mathbb{R}^{3 \\times 32 \\times 32} \\to \\mathbb{R}^2$ is then applied, followed by a softmax, so that the output may be interpreted as a length-2 tuple $(P(\\text{picture is of a cat}), P(\\text{picture is of a dog}))$.\n",
    "\n",
    "In conventional mathematical notation, this computation may be denoted\n",
    "$$\n",
    "\t\\text{softmax}\\left(\\ell_\\theta\\left(y(0) + \\int_0^T f_\\theta(t, y(t)) \\,\\mathrm{d} t\\right)\\right).\n",
    "$$\n",
    "\n",
    "The parameters of the model are $\\theta$. The computation graph may be backpropagated through and trained via stochastic gradient descent in the usual way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77905366-9443-48e8-8687-895755690d2d",
   "metadata": {},
   "source": [
    "### Neural ODEs are the continuous limit of residual networks\n",
    "\n",
    "Recall the formulation of a residual network:\n",
    "$$\n",
    "\ty_{j + 1} = y_j + f_\\theta(j, y_j),\n",
    "$$\n",
    "where $f_\\theta(j, \\,\\cdot\\,)$ is the $j$-th residual block. (The parameters of all blocks are concatenated together into $\\theta$.)\n",
    "\n",
    "Now recall the neural ODE\n",
    "$$\n",
    "\t\\frac{\\mathrm{d} y}{\\mathrm{d} t}(t) = f_\\theta(t, y(t)).\n",
    "$$\n",
    "Discretising this via the explicit Euler method at times $t_j$ uniformly separated by $\\Delta t$ gives\n",
    "$$\n",
    "\t\\frac{y(t_{j + 1}) - y(t_j)}{\\Delta t} \\approx \\frac{\\mathrm{d} y}{\\mathrm{d} t}(t_j) = f_\\theta(t_j, y(t_j)),\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "\ty(t_{j + 1}) = y({t_j}) + \\Delta t f_\\theta(t_j, y(t_j)).\n",
    "$$\n",
    "Absorbing the $\\Delta t$ into the $f_\\theta$, we recover the formulation of a residual network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa753d6c-d84a-4b54-b7b1-290dd1e1f213",
   "metadata": {},
   "source": [
    "It transpires that the key features of a GRU or an LSTM, over generic recurrent networks, are updates rules that look suspiciously like discretised differential equations. StyleGAN2 and (score based) diffusion models are simply discretised SDEs. Coupling layers in invertible neural networks turn out to be related to reversible differential equation solvers. And so on.\n",
    "\n",
    "By coincidence (or, as the idea becomes more popular, by design) many of the most effective and popular deep learning architectures resemble differential equations. Perhaps we should not be surprised: differential equations have been the dominant modelling paradigm for centuries; they are not so easily toppled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013abac-67f0-43f9-9898-13dd141e2b2b",
   "metadata": {},
   "source": [
    "### An important distinction: Physics-informed neural network\n",
    "There has been a line of work on obtaining numerical approximations to the solution $y$ of an ODE $\\frac{\\mathrm{d} y}{\\mathrm{d} t} = f(t, y(t))$ by representing the solution as some neural network $y = y_\\theta$.\n",
    "\n",
    "Perhaps $f$ is known, and the model $y_\\theta$ is fitted by minimising a loss function of the form \n",
    "$$\n",
    "\\min_\\theta \\frac{1}{N}\\sum_{i=1}^N \\left|{\\frac{\\mathrm{d} y_\\theta}{\\mathrm{d} t}(t_i) - f(t_i, y_\\theta(t_i))}\\right|\n",
    "$$\n",
    "for some points $t_i \\in [0, T]$. As such each solution to the differential equation is obtained by solving an optimisation problem. This has strong overtones of collocation methods or finite element methods.\n",
    "\n",
    "This is known as a physics-informed neural network (PINN). PINNs are effective when generalised to some PDEs, in particular nonlocal or high-dimensional PDEs, for which traditional solvers are computationally expensive. (Although in most regimes traditional solvers are still the more efficient choice.)\n",
    "\n",
    "However, we emphasise that **this is a distinct notion to neural differential equations**. NDEs use neural networks to *specify* differential equations. PINN uses neural networks to *obtain solutions to prespecified* differential equations. This distinction is a common point of confusion, especially as the PDE equivalent of PINN is sometimes referred to as a 'neural partial differential equation'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30339109-d9c6-4a77-8075-313533466102",
   "metadata": {},
   "source": [
    "### Physical modelling with inductive biases\n",
    "\n",
    "Endowing a model with any known structure of a problem is known as giving the model an **inductive bias**. 'Soft' biases through penalty terms are one common example. 'Hard' biases through explicit architectural choices are another.\n",
    "\n",
    "Physical problems often have known structure, and so a common theme has been to build in inductive biases by hybridising neural networks into this structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a406d-78e7-44cd-861a-408a2ed5b2cb",
   "metadata": {},
   "source": [
    "#### Universal differential equations\n",
    "\n",
    "Consider the Lotka-Volterra model, which is a well known approach for modelling the interaction between a predator species and a prey species:\n",
    "$$\n",
    "\t\\frac{\\mathrm{d} x}{\\mathrm{d} t}(t) = \\alpha x(t) - \\beta x(t) y(t)\n",
    "$$\n",
    "$$\n",
    "\t\\frac{\\mathrm{d} y}{\\mathrm{d} t}(t) = -\\gamma x(t) + \\delta x(t) y(t)\n",
    "$$\n",
    "Here, $x(t) \\in \\mathbb{R}$ and $y(t) \\in \\mathbb{R}$ represent the size of the population of the prey and predator species respectively, at each time $t \\in [0, T]$. The right hand side is theoretically constructed, representing interactions between these species.\n",
    "\n",
    "This theory will not usually be perfectly accurate, however. There will be some gap between the theoretical prediction and what is observed in practice. To remedy this, and letting $f_\\theta, g_\\theta \\colon \\mathbb{R}^2 \\to \\mathbb{R}$ be neural networks, we may instead consider the model\n",
    "$$\n",
    "\t\\frac{\\mathrm{d} x}{\\mathrm{d} t}(t) = \\alpha x(t) - \\beta x(t) y(t) + f_\\theta(x(t), y(t))\n",
    "$$\n",
    "$$\n",
    "\t\\frac{\\mathrm{d} y}{\\mathrm{d} t}(t) = -\\gamma x(t) + \\delta x(t) y(t) + g_\\theta(x(t), y(t))\n",
    "$$\n",
    "in which an existing theoretical model is augmented with a neural network correction term.\n",
    "\n",
    "We broadly refer to this approach as a **universal differential equation**. (There is little unified terminology here. Other authors have considered essentially the same idea under other names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9aa45-72ca-44ec-8d9a-95693cf0e288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
